{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "499cdee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to load weight to spesific models\n",
    "\"\"\"\n",
    "def backbone_load(model,model_weight,keys=['input_conv','unet','offset_linear', 'output_layer', 'semantic_linear'],debugg=False):\n",
    "    processed_dict = {}\n",
    "    for k in net_weighth.keys(): \n",
    "        decomposed_key = k.split(\".\")[0]\n",
    "        if(decomposed_key in keys):\n",
    "            processed_dict[k] = net_weighth[k] \n",
    "    if debugg :\n",
    "        return processed_dict\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_dict = {k: v for k, v in processed_dict.items() if k in model_dict}\n",
    "    model_dict.update(pretrained_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fe652fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/dev-thesis2\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "343644f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from model import SoftGroup\n",
    "from model import HAIS\n",
    "import yaml\n",
    "from munch import Munch\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d8eb36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.load('tensors.pt', map_location=torch.device('cpu'))\n",
    "data = torch.load(\"backbone_work_dirs/unet_100x100_ASPP_DICELOSS/epoch_20.pth\")\n",
    "net_weighth= data[\"net\"]\n",
    "cfg_txt = open(\"configs/HAIS_stpls3d.yaml\", 'r').read()\n",
    "cfg_txt_sofggroup = open(\"configs/training_backbone/50x50_50/unet_ASPP_DICELOSS.yaml\", 'r').read()\n",
    "\n",
    "cfg = Munch.fromDict(yaml.safe_load(cfg_txt))\n",
    "cfg_softgroup = Munch.fromDict(yaml.safe_load(cfg_txt_sofggroup))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b74a2ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import is_dataclass\n",
    "import functools\n",
    "\n",
    "import spconv.pytorch as spconv\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from lib.hais_ops.functions import hais_ops\n",
    "from util import cuda_cast, force_fp32, rle_encode, DiceLoss\n",
    "from model.blocks import MLP, ResidualBlock, UNET,UNET_ASPP\n",
    "\n",
    "\"\"\" \n",
    "DELETE cls \n",
    "UPDATE DATA CLUSTRING PARTS\n",
    "AND MAKE SURE THE OPS PART\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class HAIS(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 channels=32,\n",
    "                 num_blocks=7,\n",
    "                 semantic_only=False,\n",
    "                 semantic_classes=20,\n",
    "                 instance_classes=18,\n",
    "                 semantic_weight=None,\n",
    "                 sem2ins_classes=[],\n",
    "                 ignore_label=-100,\n",
    "                 with_coords=True,\n",
    "                 grouping_cfg=None,\n",
    "                 instance_voxel_cfg=None,\n",
    "                 train_cfg=None,\n",
    "                 test_cfg=None,\n",
    "                 fixed_modules=[],\n",
    "                 hais_util=None,\n",
    "                 modified_unet=None):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.num_blocks = num_blocks\n",
    "        self.semantic_only = semantic_only\n",
    "        self.semantic_classes = semantic_classes\n",
    "        self.instance_classes = instance_classes\n",
    "        self.semantic_weight = semantic_weight\n",
    "        self.sem2ins_classes = sem2ins_classes\n",
    "        self.ignore_label = ignore_label\n",
    "        self.with_coords = with_coords\n",
    "        self.grouping_cfg = grouping_cfg\n",
    "        self.instance_voxel_cfg = instance_voxel_cfg\n",
    "        self.train_cfg = train_cfg\n",
    "        self.test_cfg = test_cfg\n",
    "        self.fixed_modules = fixed_modules\n",
    "        self.hais_util = hais_util\n",
    "        self.modified_unet = modified_unet\n",
    "        \n",
    "\n",
    "        block = ResidualBlock\n",
    "        norm_fn = functools.partial(nn.BatchNorm1d, eps=1e-4, momentum=0.1)\n",
    "        \n",
    "        # backbone\n",
    "        in_channels = 6 if with_coords else 3\n",
    "        self.input_conv = spconv.SparseSequential(\n",
    "            spconv.SubMConv3d(\n",
    "                in_channels, channels, kernel_size=3, padding=1, bias=False, indice_key='subm1'))\n",
    "        block_channels = [channels * (i + 1) for i in range(num_blocks)]\n",
    "        \"ASPP flag\"\n",
    "        is_ASPP =  getattr(self.modified_unet, 'ASPP', False) if self.modified_unet != None else False\n",
    "        if is_ASPP:\n",
    "            self.unet = UNET_ASPP(block_channels,norm_fn,2)\n",
    "        else:\n",
    "            self.unet = UNET(block_channels, norm_fn, 2, block, indice_key_id=1)\n",
    "        \n",
    "        self.output_layer = spconv.SparseSequential(norm_fn(channels), nn.ReLU()).cuda()\n",
    "\n",
    "        # point-wise prediction\n",
    "        self.semantic_linear = MLP(channels, semantic_classes, norm_fn=norm_fn, num_layers=2).cuda()\n",
    "        self.offset_linear = MLP(channels, 3, norm_fn=norm_fn, num_layers=2).cuda()\n",
    "\n",
    "        # topdown refinement path\n",
    "        if not semantic_only:\n",
    "            self.tiny_unet = UNET([channels, 2 * channels], norm_fn, 2, block, indice_key_id=11)\n",
    "            self.tiny_unet_outputlayer = spconv.SparseSequential(norm_fn(channels), nn.ReLU())\n",
    "#             self.cls_linear = nn.Linear(channels, instance_classes + 1)\n",
    "            self.mask_linear = MLP(channels, 1, norm_fn=None, num_layers=2)\n",
    "            self.iou_score_linear = nn.Linear(channels,  1) #score\n",
    "\n",
    "        self.init_weights()\n",
    "        if self.semantic_weight:\n",
    "            self.semantic_weight = torch.tensor(self.semantic_weight, dtype=torch.float, device='cuda')\n",
    "        else:\n",
    "            self.semantic_weight = None\n",
    "        self.is_DICELOSS =  getattr(self.modified_unet, 'DICELOSS', True) if self.modified_unet != None else False\n",
    "        if  self.is_DICELOSS:\n",
    "            self.diceLoss = DiceLoss( weight=self.semantic_weight,ignore_index=self.ignore_label).cuda()\n",
    "        for mod in fixed_modules:\n",
    "            mod = getattr(self, mod)\n",
    "            for param in mod.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, MLP):\n",
    "                m.init_weights()\n",
    "        if not self.semantic_only:\n",
    "            for m in [self.iou_score_linear]:\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        super().train(mode)\n",
    "        for mod in self.fixed_modules:\n",
    "            mod = getattr(self, mod)\n",
    "            for m in mod.modules():\n",
    "                if isinstance(m, nn.BatchNorm1d):\n",
    "                    m.eval()\n",
    "\n",
    "    def forward(self, batch,current_epoch,return_loss=False):\n",
    "        if return_loss:\n",
    "            return self.forward_train(current_epoch,**batch)\n",
    "        else:\n",
    "            return self.forward_test(current_epoch,**batch)\n",
    "\n",
    "    #@cuda_cast\n",
    "    def forward_train(self, epoch, batch_idxs, voxel_coords, p2v_map, v2p_map, coords_float, feats,\n",
    "                      semantic_labels, instance_labels, instance_pointnum, instance_cls,\n",
    "                      pt_offset_labels, spatial_shape, batch_size, **kwargs):\n",
    "        losses = {}\n",
    "        if self.with_coords:\n",
    "            feats = torch.cat((feats, coords_float), 1)\n",
    "        voxel_feats = hais_ops.voxelization(feats, p2v_map)\n",
    "        input = spconv.SparseConvTensor(voxel_feats, voxel_coords.int(), spatial_shape, batch_size)\n",
    "        semantic_scores, pt_offsets, output_feats = self.forward_backbone(input, v2p_map)\n",
    "        #return semantic_scores, pt_offsets, output_feats     \n",
    "        # point wise losses\n",
    "        # Semantic segmentation and centroid loss.\n",
    "        point_wise_loss = self.point_wise_loss(semantic_scores, pt_offsets, semantic_labels,\n",
    "                                               instance_labels, pt_offset_labels)\n",
    "        losses.update(point_wise_loss)\n",
    "        \n",
    "        # instance losses\n",
    "        if not self.semantic_only:\n",
    "            proposals_idx, proposals_offset = self.forward_grouping(semantic_scores, pt_offsets,\n",
    "                                                                    batch_idxs, coords_float)\n",
    "            if proposals_offset.shape[0] > self.train_cfg.max_proposal_num:\n",
    "                proposals_offset = proposals_offset[:self.train_cfg.max_proposal_num + 1]\n",
    "                proposals_idx = proposals_idx[:proposals_offset[-1]]\n",
    "                assert proposals_idx.shape[0] == proposals_offset[-1]\n",
    "            inst_feats, inst_map = self.clusters_voxelization(\n",
    "                proposals_idx,\n",
    "                proposals_offset,\n",
    "                output_feats,\n",
    "                coords_float,\n",
    "                rand_quantize=True,\n",
    "                **self.instance_voxel_cfg)\n",
    "            \n",
    "            iou_scores, mask_scores = self.forward_instance(inst_feats, inst_map,proposals_offset,epoch)\n",
    "            instance_loss = self.instance_loss( mask_scores, iou_scores, proposals_idx,\n",
    "                                               proposals_offset, instance_labels, instance_pointnum,epoch)\n",
    "            losses.update(instance_loss)\n",
    "        return self.parse_losses(losses)\n",
    "\n",
    "    def point_wise_loss(self, semantic_scores, pt_offsets, semantic_labels, instance_labels,\n",
    "                        pt_offset_labels):\n",
    "        losses = {}\n",
    "                \n",
    "        if self.is_DICELOSS:\n",
    "            semantic_loss = self.diceloss(semantic_scores, semantic_labels)\n",
    "        else:\n",
    "            semantic_loss = F.cross_entropy(\n",
    "                semantic_scores, semantic_labels, weight=self.semantic_weight, ignore_index=self.ignore_label)\n",
    "        losses['semantic_loss'] = semantic_loss\n",
    "\n",
    "        pos_inds = instance_labels != self.ignore_label\n",
    "        if pos_inds.sum() == 0:\n",
    "            offset_loss = 0 * pt_offsets.sum()\n",
    "        else:\n",
    "            offset_loss = F.l1_loss(\n",
    "                pt_offsets[pos_inds], pt_offset_labels[pos_inds], reduction='sum') / pos_inds.sum()\n",
    "        losses['offset_loss'] = offset_loss\n",
    "        return losses\n",
    "\n",
    "    #@force_fp32(apply_to=('mask_scores', 'iou_scores'))\n",
    "    def instance_loss(self, mask_scores, iou_scores, proposals_idx, proposals_offset,\n",
    "                      instance_labels, instance_pointnum,epoch):\n",
    "        losses = {}\n",
    "        mask_scores_sigmoid = torch.sigmoid(mask_scores)\n",
    "\n",
    "        if getattr(self.hais_util, 'cal_iou_based_on_mask', False) \\\n",
    "                and (epoch > self.hais_util.cal_iou_based_on_mask_start_epoch):\n",
    "            ious, mask_label =  hais_ops.cal_iou_and_masklabel(proposals_idx[:, 1].cuda(), \\\n",
    "                proposals_offset.cuda(), instance_labels, instance_pointnum, mask_scores_sigmoid.detach(), 1)\n",
    "        else:\n",
    "            ious, mask_label =  hais_ops.cal_iou_and_masklabel(proposals_idx[:, 1].cuda(), \\\n",
    "                proposals_offset.cuda(), instance_labels, instance_pointnum, mask_scores_sigmoid.detach(), 0)\n",
    "        # ious: (nProposal, nInstance)\n",
    "        # mask_label: (sumNPoint, 1)\n",
    "\n",
    "        mask_label_weight = (mask_label != -1).float()\n",
    "        mask_label[mask_label==-1.] = 0.5 # any value is ok\n",
    "        mask_loss = torch.nn.functional.binary_cross_entropy(mask_scores_sigmoid, mask_label, weight=mask_label_weight, reduction='none')\n",
    "        mask_loss = mask_loss.mean()\n",
    "        #losses['mask_loss'] = (mask_loss, mask_label_weight.sum())\n",
    "        losses['mask_loss'] = mask_loss\n",
    "        \n",
    "        gt_ious, _ = ious.max(1)  # gt_ious: (nProposal) float, long\n",
    "            \n",
    "\n",
    "        gt_scores = self.get_segmented_scores(gt_ious, self.hais_util.fg_thresh, self.hais_util.bg_thresh)\n",
    "\n",
    "        score_loss = F.binary_cross_entropy(torch.sigmoid(iou_scores.view(-1)), gt_scores, reduction =\"none\")\n",
    "        score_loss = score_loss.mean()\n",
    "\n",
    "        #losses['score_loss'] = (score_loss, gt_ious.shape[0])\n",
    "        losses['score_loss'] = score_loss\n",
    "    \n",
    "        return losses\n",
    "\n",
    "    def parse_losses(self, losses):\n",
    "        loss = sum(v for v in losses.values())\n",
    "        losses['loss'] = loss\n",
    "        for loss_name, loss_value in losses.items():\n",
    "            if dist.is_available() and dist.is_initialized():\n",
    "                loss_value = loss_value.data.clone()\n",
    "                dist.all_reduce(loss_value.div_(dist.get_world_size()))\n",
    "            losses[loss_name] = loss_value.item()\n",
    "        return loss, losses\n",
    "\n",
    "    #@cuda_cast\n",
    "    def forward_test(self, epoch,batch_idxs, voxel_coords, p2v_map, v2p_map, coords_float, feats,\n",
    "                     semantic_labels, instance_labels, pt_offset_labels, spatial_shape, batch_size,\n",
    "                     scan_ids, **kwargs):\n",
    "        color_feats = feats\n",
    "        if self.with_coords:\n",
    "            feats = torch.cat((feats, coords_float), 1)\n",
    "        voxel_feats = hais_ops.voxelization(feats, p2v_map)\n",
    "        input = spconv.SparseConvTensor(voxel_feats, voxel_coords.int(), spatial_shape, batch_size)\n",
    "        semantic_scores, pt_offsets, output_feats = self.forward_backbone(\n",
    "            input, v2p_map)\n",
    "        semantic_preds = semantic_scores.max(1)[1]\n",
    "        ret = dict(\n",
    "            scan_id=scan_ids[0],\n",
    "            coords_float=coords_float.cpu().numpy(),\n",
    "            color_feats=color_feats.cpu().numpy(),\n",
    "            semantic_preds=semantic_preds.cpu().numpy(),\n",
    "            semantic_labels=semantic_labels.cpu().numpy(),\n",
    "            offset_preds=pt_offsets.cpu().numpy(),\n",
    "            offset_labels=pt_offset_labels.cpu().numpy(),\n",
    "            instance_labels=instance_labels.cpu().numpy())\n",
    "        if not self.semantic_only:\n",
    "            proposals_idx, proposals_offset = self.forward_grouping(semantic_scores, pt_offsets,\n",
    "                                                                    batch_idxs, coords_float,'test')\n",
    "            inst_feats, inst_map = self.clusters_voxelization(proposals_idx, proposals_offset,\n",
    "                                                              output_feats, coords_float,\n",
    "                                                              **self.instance_voxel_cfg)\n",
    "            iou_scores, mask_scores = self.forward_instance(inst_feats, inst_map,proposals_offset,epoch)\n",
    "            pred_instances = self.get_instances(scan_ids[0], proposals_offset,proposals_idx, semantic_scores, iou_scores, mask_scores,N=feats.shape[0])\n",
    "            gt_instances = self.get_gt_instances(semantic_labels, instance_labels)\n",
    "            ret.update(dict(pred_instances=pred_instances, gt_instances=gt_instances))\n",
    "        return ret\n",
    "\n",
    "    def forward_backbone(self, input, input_map):\n",
    "        output = self.input_conv(input)\n",
    "        output = self.unet(output)\n",
    "        output = self.output_layer(output)\n",
    "        output_feats = output.features[input_map.long()]\n",
    "\n",
    "        semantic_scores = self.semantic_linear(output_feats)\n",
    "        pt_offsets = self.offset_linear(output_feats)\n",
    "        return semantic_scores, pt_offsets, output_feats\n",
    "\n",
    "    #@force_fp32(apply_to=('semantic_scores, pt_offsets'))\n",
    "    def forward_grouping(self,semantic_scores,pt_offsets,batch_idxs,coords_float,training_mode='train'):\n",
    "\n",
    "        batch_size = batch_idxs.max() + 1\n",
    "        semantic_preds = semantic_scores.max(1)[1]\n",
    "\n",
    "        radius = self.grouping_cfg.radius\n",
    "        mean_active = self.grouping_cfg.mean_active\n",
    "        object_idxs = torch.nonzero(semantic_preds > 0).view(-1) # floor idx 0, wall idx 1\n",
    " \n",
    "        batch_idxs_ = batch_idxs[object_idxs]\n",
    "        batch_offsets_ = self.get_batch_offsets(batch_idxs_, batch_size)\n",
    "        coords_ = coords_float[object_idxs]\n",
    "        pt_offsets_ = pt_offsets[object_idxs]\n",
    "        semantic_scores_cpu = semantic_scores[object_idxs].int().cpu()\n",
    "        \n",
    "        idx, start_len = hais_ops.ballquery_batch_p(coords_ + pt_offsets_, batch_idxs_, batch_offsets_,\n",
    "                                           radius, mean_active)\n",
    "        if training_mode ==\"train\":\n",
    "            using_set_aggr = getattr(self.hais_util, 'using_set_aggr_in_training', True)\n",
    "        else:\n",
    "            using_set_aggr = getattr(self.hais_util, 'using_set_aggr_in_testing', True)\n",
    "        proposals_idx, proposals_offset = hais_ops.hierarchical_aggregation(semantic_scores_cpu, (coords_ + pt_offsets_).cpu(), \n",
    "                                                                            idx.cpu(), start_len.cpu(), batch_idxs_.cpu(), \n",
    "                                                                            training_mode, using_set_aggr)             \n",
    "        proposals_idx[:, 1] = object_idxs[proposals_idx[:, 1].long()].int()        \n",
    "\n",
    "        return proposals_idx, proposals_offset\n",
    "\n",
    "    def forward_instance(self, inst_feats, inst_map,proposals_offset,epoch):\n",
    "        feats = self.tiny_unet(inst_feats)\n",
    "        feats = self.tiny_unet_outputlayer(feats)\n",
    "        score_feats = feats.features[inst_map.long()]\n",
    "\n",
    "        mask_scores = self.mask_linear(feats.features)\n",
    "        mask_scores = mask_scores[inst_map.long()]\n",
    "\n",
    "        if getattr(self.hais_util, 'use_mask_filter_score_feature', False)  and \\\n",
    "                    epoch > self.hais_util.use_mask_filter_score_feature_start_epoch:\n",
    "            mask_index_select = torch.ones_like(mask_scores)\n",
    "            mask_index_select[torch.sigmoid(mask_scores) < self.hais_util.mask_filter_score_feature_thre] = 0.\n",
    "            score_feats = score_feats * mask_index_select\n",
    "        score_feats = hais_ops.roipool(score_feats, proposals_offset.cuda())  # (nProposal, C)\n",
    "        iou_scores = self.iou_score_linear(score_feats)  # (nProposal, 1)\n",
    "#         iou_scores = self.iou_score_linear(feats)\n",
    "\n",
    "        return iou_scores, mask_scores\n",
    "\n",
    "    #@force_fp32(apply_to=('semantic_scores', 'iou_scores', 'mask_scores'))\n",
    "    def get_instances(self, scan_id, \n",
    "                      proposals_offset, \n",
    "                      proposals_idx, \n",
    "                      semantic_scores, \n",
    "                      iou_scores, \n",
    "                      mask_scores,\n",
    "                      N,\n",
    "                      semantic_label_idx=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]):\n",
    "        # num_instances = cls_scores.size(0)\n",
    "        # num_points = semantic_scores.size(0)\n",
    "        # cls_scores = cls_scores.softmax(1)\n",
    "        semantic_pred = semantic_scores.max(1)[1]\n",
    "        scores_pred = torch.sigmoid(iou_scores.view(-1))\n",
    "        proposals_pred = torch.zeros((proposals_offset.shape[0] - 1, N), dtype=torch.int, device=scores_pred.device)\n",
    "        test_mask_score_thre = self.test_cfg.mask_score_thr #getattr(cfg, 'test_mask_score_thre', -0.5)\n",
    "        _mask = mask_scores.squeeze(1) > test_mask_score_thre\n",
    "        proposals_pred[proposals_idx[_mask][:, 0].long(), proposals_idx[_mask][:, 1].long()] = 1\n",
    "        ## semantic_label_idx => semantic label \n",
    "        semantic_id = torch.tensor(semantic_label_idx, device=scores_pred.device)[semantic_pred[proposals_idx[:, 1][proposals_offset[:-1].long()].long()]] # (nProposal), long\n",
    "        # semantic_id_idx = semantic_pred[proposals_idx[:, 1][proposals_offset[:-1].long()].long()]\n",
    "        score_mask = (scores_pred > self.test_cfg.cls_score_thr) #cfg.TEST_SCORE_THRESH\n",
    "        scores_pred = scores_pred[score_mask]\n",
    "        proposals_pred = proposals_pred[score_mask]\n",
    "        semantic_id = semantic_id[score_mask]\n",
    "                # semantic_id_idx = semantic_id_idx[score_mask]\n",
    "\n",
    "                # npoint threshold\n",
    "        proposals_pointnum = proposals_pred.sum(1)\n",
    "        npoint_mask = (proposals_pointnum >= self.test_cfg.min_npoint) #cfg.TEST_NPOINT_THRESH)\n",
    "        clusters = scores_pred[npoint_mask]\n",
    "        cluster_scores = proposals_pred[npoint_mask]\n",
    "        cluster_semantic_id = semantic_id[npoint_mask]\n",
    "        # nclusters = clusters.shape[0]\n",
    "        cls_pred = cluster_semantic_id.cpu().numpy()\n",
    "        score_pred = clusters.cpu().detach().numpy()\n",
    "        mask_pred = cluster_scores.cpu().numpy()\n",
    "        instances = []\n",
    "        for i in range(cls_pred.shape[0]):\n",
    "            pred = {}\n",
    "            pred['scan_id'] = scan_id\n",
    "            pred['label_id'] = cls_pred[i]\n",
    "            pred['conf'] = score_pred[i]\n",
    "            # rle encode mask to save memory\n",
    "            pred['pred_mask'] = rle_encode(mask_pred[i])\n",
    "            instances.append(pred)\n",
    "        return instances\n",
    "\n",
    "    def get_gt_instances(self, semantic_labels, instance_labels):\n",
    "        \"\"\"Get gt instances for evaluation.\"\"\"\n",
    "        # convert to evaluation format 0: ignore, 1->N: valid\n",
    "        label_shift = self.semantic_classes - self.instance_classes\n",
    "        semantic_labels = semantic_labels - label_shift + 1\n",
    "        semantic_labels[semantic_labels < 0] = 0\n",
    "        instance_labels += 1\n",
    "        ignore_inds = instance_labels < 0\n",
    "        # scannet encoding rule\n",
    "        gt_ins = semantic_labels * 1000 + instance_labels\n",
    "        gt_ins[ignore_inds] = 0\n",
    "        gt_ins = gt_ins.cpu().numpy()\n",
    "        return gt_ins\n",
    "\n",
    "    #@force_fp32(apply_to='feats')\n",
    "    def clusters_voxelization(self,\n",
    "                              clusters_idx,\n",
    "                              clusters_offset,\n",
    "                              feats,\n",
    "                              coords,\n",
    "                              scale,\n",
    "                              spatial_shape,\n",
    "                              rand_quantize=False):\n",
    "        batch_idx = clusters_idx[:, 0].cuda().long()\n",
    "        c_idxs = clusters_idx[:, 1].cuda()\n",
    "        feats = feats[c_idxs.long()]\n",
    "        coords = coords[c_idxs.long()]\n",
    "\n",
    "        coords_min = hais_ops.sec_min(coords, clusters_offset.cuda())\n",
    "        coords_max = hais_ops.sec_max(coords, clusters_offset.cuda())\n",
    "\n",
    "        # 0.01 to ensure voxel_coords < spatial_shape\n",
    "        clusters_scale = 1 / ((coords_max - coords_min) / spatial_shape).max(1)[0] - 0.01\n",
    "        clusters_scale = torch.clamp(clusters_scale, min=None, max=scale)\n",
    "\n",
    "        coords_min = coords_min * clusters_scale[:, None]\n",
    "        coords_max = coords_max * clusters_scale[:, None]\n",
    "        clusters_scale = clusters_scale[batch_idx]\n",
    "        coords = coords * clusters_scale[:, None]\n",
    "\n",
    "        if rand_quantize:\n",
    "            # after this, coords.long() will have some randomness\n",
    "            range = coords_max - coords_min\n",
    "            coords_min -= torch.clamp(spatial_shape - range - 0.001, min=0) * torch.rand(3).cuda()\n",
    "            coords_min -= torch.clamp(spatial_shape - range + 0.001, max=0) * torch.rand(3).cuda()\n",
    "        coords_min = coords_min[batch_idx]\n",
    "        coords -= coords_min\n",
    "        \"\"\"\n",
    "        NOTE \n",
    "        BELLOW SAME LIKE HAIS*\n",
    "        \"\"\"\n",
    "        assert coords.shape.numel() == ((coords >= 0) * (coords < spatial_shape)).sum()\n",
    "        coords = coords.long()\n",
    "        coords = torch.cat([clusters_idx[:, 0].view(-1, 1).long(), coords.cpu()], 1)\n",
    "\n",
    "        out_coords, inp_map, out_map = hais_ops.voxelization_idx(coords, int(clusters_idx[-1, 0]) + 1)\n",
    "        out_feats = hais_ops.voxelization(feats, out_map.cuda())\n",
    "        spatial_shape = [spatial_shape] * 3\n",
    "        voxelization_feats = spconv.SparseConvTensor(out_feats,\n",
    "                                                     out_coords.int().cuda(), spatial_shape,\n",
    "                                                     int(clusters_idx[-1, 0]) + 1)\n",
    "        return voxelization_feats, inp_map\n",
    "\n",
    "    def get_batch_offsets(self, batch_idxs, bs):\n",
    "        batch_offsets = torch.zeros(bs + 1).int().cuda()\n",
    "        for i in range(bs):\n",
    "            batch_offsets[i + 1] = batch_offsets[i] + (batch_idxs == i).sum()\n",
    "        assert batch_offsets[-1] == batch_idxs.shape[0]\n",
    "        return batch_offsets\n",
    "    \n",
    "    def get_segmented_scores(self,scores, fg_thresh=1.0, bg_thresh=0.0):\n",
    "        '''\n",
    "        :param scores: (N), float, 0~1\n",
    "        :return: segmented_scores: (N), float 0~1, >fg_thresh: 1, <bg_thresh: 0, mid: linear\n",
    "        '''\n",
    "        fg_mask = scores > fg_thresh\n",
    "        bg_mask = scores < bg_thresh\n",
    "        interval_mask = (fg_mask == 0) & (bg_mask == 0)\n",
    "\n",
    "        segmented_scores = (fg_mask > 0).float()\n",
    "        k = 1 / (fg_thresh - bg_thresh + 1e-5)\n",
    "        b = bg_thresh / (bg_thresh - fg_thresh + 1e-5)\n",
    "        segmented_scores[interval_mask] = scores[interval_mask] * k + b\n",
    "\n",
    "        return segmented_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54d92266",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HAIS(**cfg_softgroup.model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8558b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = backbone_load(model,net_weighth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebcd6cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import build_dataloader, build_dataset\n",
    "from evaluation import (ScanNetEval, evaluate_offset_mae, evaluate_semantic_acc,\n",
    "                                  evaluate_semantic_miou)\n",
    "from model import SoftGroup\n",
    "from util import (AverageMeter, SummaryWriter, build_optimizer, checkpoint_save,\n",
    "                  collect_results_gpu, cosine_lr_after_step, get_dist_info,\n",
    "                  get_max_memory, get_root_logger, init_dist, is_main_process,\n",
    "                  is_multiple, is_power2, load_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e78c3e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-29 10:34:50,728 - INFO - Config:\n",
      "model:\n",
      "  channels: 16\n",
      "  num_blocks: 7\n",
      "  semantic_classes: 15\n",
      "  instance_classes: 14\n",
      "  sem2ins_classes: []\n",
      "  semantic_only: True\n",
      "  semantic_weight: [1.0, 1.0, 44.0, 21.9, 1.8, 25.1, 31.5, 21.8, 24.0, 54.4, 114.4,\n",
      "                    81.2, 43.6, 9.7, 22.4]\n",
      "  ignore_label: -100\n",
      "  with_coords: False\n",
      "  grouping_cfg:\n",
      "    score_thr: 0.2\n",
      "    radius: 0.9\n",
      "    mean_active: 3\n",
      "    class_numpoint_mean: [-1., 10408., 58., 124., 1351., 162., 430., 1090., 451., 26., 43.,\n",
      "                          61., 39., 109., 1239]\n",
      "    npoint_thr: 0.05  # absolute if class_numpoint == -1, relative if class_numpoint != -1\n",
      "    ignore_classes: [0]\n",
      "  instance_voxel_cfg:\n",
      "    scale: 3\n",
      "    spatial_shape: 20\n",
      "  train_cfg:\n",
      "    max_proposal_num: 300\n",
      "    pos_iou_thr: 0.5\n",
      "    match_low_quality: True\n",
      "    min_pos_thr: 0.1\n",
      "  test_cfg:\n",
      "    x4_split: False\n",
      "    cls_score_thr: 0.001\n",
      "    mask_score_thr: -0.5\n",
      "    min_npoint: 15\n",
      "  fixed_modules: []\n",
      "  hais_util:  \n",
      "    cal_iou_based_on_mask: true\n",
      "    cal_iou_based_on_mask_start_epoch: 200\n",
      "    use_mask_filter_score_feature: true\n",
      "    use_mask_filter_score_feature_start_epoch: 200\n",
      "    using_set_aggr_in_testing: true\n",
      "    using_set_aggr_in_training: false\n",
      "    mask_filter_score_feature_thre: 0.5\n",
      "    using_set_aggr_in_training: False\n",
      "    using_set_aggr_in_testing: True\n",
      "    fg_thresh: 1.\n",
      "    bg_thresh: 0.\n",
      "  modified_unet:\n",
      "    ASPP: True\n",
      "    DICELOSS: True\n",
      "data:\n",
      "  train:\n",
      "    type: 'stpls3d'\n",
      "    data_root: 'dataset/stpls3d'\n",
      "    prefix: 'train'\n",
      "    suffix: '_inst_nostuff.pth'\n",
      "    training: True\n",
      "    repeat: 4\n",
      "    voxel_cfg:\n",
      "      scale: 3\n",
      "      spatial_shape: [128, 512]\n",
      "      max_npoint: 250000\n",
      "      min_npoint: 5000\n",
      "  test:\n",
      "    type: 'stpls3d'\n",
      "    data_root: 'dataset/stpls3d'\n",
      "    prefix: 'val'\n",
      "    suffix: '_inst_nostuff.pth'\n",
      "    training: False\n",
      "    voxel_cfg:\n",
      "      scale: 3\n",
      "      spatial_shape: [128, 512]\n",
      "      max_npoint: 250000\n",
      "      min_npoint: 5000\n",
      "\n",
      "dataloader:\n",
      "  train:\n",
      "    batch_size: 4\n",
      "    num_workers: 4\n",
      "  test:\n",
      "    batch_size: 1\n",
      "    num_workers: 1\n",
      "\n",
      "optimizer:\n",
      "  type: 'Adam'\n",
      "  lr: 0.004\n",
      "\n",
      "save_cfg:\n",
      "  semantic: True\n",
      "  offset: True\n",
      "  instance: True\n",
      "\n",
      "eval_min_npoint: 10\n",
      "\n",
      "fp16: False\n",
      "epochs: 108\n",
      "step_epoch: 20\n",
      "save_freq: 4\n",
      "pretrain: './work_dirs/softgroup_stpls3d_backbone/latest.pth'\n",
      "work_dir: ''\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "import os\n",
    "import time\n",
    "\n",
    "cfg.work_dir = osp.join('./work_dirs', osp.splitext(osp.basename(\"test\"))[0])\n",
    "os.makedirs(osp.abspath(cfg.work_dir), exist_ok=True)\n",
    "timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())\n",
    "log_file = osp.join(cfg.work_dir, f'{timestamp}.log')\n",
    "logger = get_root_logger(log_file=log_file)\n",
    "logger.info(f'Config:\\n{cfg_txt}')\n",
    "# logger.info(f'Distributed: {args.dist}')\n",
    "# logger.info(f'Mix precision training: {cfg.fp16}')\n",
    "# shutil.copy(args.config, osp.join(cfg.work_dir, osp.basename(args.config)))\n",
    "writer = SummaryWriter(cfg.work_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70c006af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set = build_dataset(cfg.data.train, logger)\n",
    "# val_set = build_dataset(cfg.data.test, logger)\n",
    "# train_loader = build_dataloader(\n",
    "# train_set, training=True, dist=False, **cfg.dataloader.train)\n",
    "# val_loader = build_dataloader(val_set, training=False, dist=False, **cfg.dataloader.test)\n",
    "# for batch in train_set:\n",
    "#     data_batch = batch\n",
    "#     break\n",
    "# torch.save(data_batch, 'dummy_data.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04a4ccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in val_loader:\n",
    "#     data_batch = batch\n",
    "#     break\n",
    "# torch.save(data_batch, 'dummy_data_test.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31bbec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(\"dummy_data.pth\",map_location=\"cuda:0\")\n",
    "data_test = torch.load(\"dummy_data_test.pth\",map_location=\"cuda:0\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00232538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Munch({'ASPP': True, 'DICELOSS': True})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.modified_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69504547",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HAIS' object has no attribute 'diceloss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8882/1298939936.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8882/1942610699.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch, current_epoch, return_loss)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurrent_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8882/1942610699.py\u001b[0m in \u001b[0;36mforward_train\u001b[0;34m(self, epoch, batch_idxs, voxel_coords, p2v_map, v2p_map, coords_float, feats, semantic_labels, instance_labels, instance_pointnum, instance_cls, pt_offset_labels, spatial_shape, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# point wise losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# Semantic segmentation and centroid loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         point_wise_loss = self.point_wise_loss(semantic_scores, pt_offsets, semantic_labels,\n\u001b[0m\u001b[1;32m    143\u001b[0m                                                instance_labels, pt_offset_labels)\n\u001b[1;32m    144\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoint_wise_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8882/1942610699.py\u001b[0m in \u001b[0;36mpoint_wise_loss\u001b[0;34m(self, semantic_scores, pt_offsets, semantic_labels, instance_labels, pt_offset_labels)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_DICELOSS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0msemantic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiceloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msemantic_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msemantic_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             semantic_loss = F.cross_entropy(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1183\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1186\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HAIS' object has no attribute 'diceloss'"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "batch = model(data, current_epoch=20,return_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5980552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spconv.pytorch as spconv\n",
    "from lib.hais_ops.functions import hais_ops\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2cb5586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec40fe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch =21 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceb2417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01a93a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = {}\n",
    "# if model.with_coords:\n",
    "#     feats = torch.cat((data[\"feats\"], data[\"coords_float\"]), 1)\n",
    "# voxel_feats = hais_ops.voxelization(data[\"feats\"], data[\"p2v_map\"])\n",
    "# input_buff = spconv.SparseConvTensor(voxel_feats, data[\"voxel_coords\"].int(), data[\"spatial_shape\"], data[\"batch_size\"])\n",
    "# semantic_scores, pt_offsets, output_feats = model.forward_backbone(input_buff, data[\"v2p_map\"])\n",
    "# point_wise_loss = model.point_wise_loss(semantic_scores, pt_offsets, data[\"semantic_labels\"],\n",
    "#                                        data[\"instance_labels\"], data[\"pt_offset_labels\"])\n",
    "# losses.update(point_wise_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fc8700d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic_scores\n",
    "# pt_offsets\n",
    "# batch_idxs = data[\"batch_idxs\"]\n",
    "# coords_float = data[\"coords_float\"]\n",
    "# training_mode='train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01b3c949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def forward_grouping(model,semantic_scores,pt_offsets,batch_idxs,coords_float,training_mode='train'):\n",
    "# batch_size = batch_idxs.max() + 1\n",
    "# semantic_preds = semantic_scores.max(1)[1]\n",
    "\n",
    "# radius = model.grouping_cfg.radius\n",
    "# mean_active = model.grouping_cfg.mean_active\n",
    "# object_idxs = torch.nonzero(semantic_preds > 0).view(-1) # floor idx 0, wall idx 1\n",
    " \n",
    "# batch_idxs_ = batch_idxs[object_idxs]\n",
    "# batch_offsets_ = model.get_batch_offsets(batch_idxs_, batch_size)\n",
    "# coords_ = coords_float[object_idxs]\n",
    "# pt_offsets_ = pt_offsets[object_idxs]\n",
    "# semantic_scores_cpu = semantic_scores[object_idxs].int().cpu()\n",
    "\n",
    "# idx, start_len = hais_ops.ballquery_batch_p(coords_ + pt_offsets_, batch_idxs_, batch_offsets_,radius, mean_active)\n",
    "# if training_mode ==\"train\":\n",
    "#     using_set_aggr = getattr(model.hais_util, 'using_set_aggr_in_training', True)\n",
    "# else:\n",
    "#     using_set_aggr = getattr(model.hais_util, 'using_set_aggr_in_testing', True)\n",
    "# proposals_idx, proposals_offset = hais_ops.hierarchical_aggregation(semantic_scores_cpu, (coords_ + pt_offsets_).cpu(), \n",
    "#                                                                     idx.cpu(), start_len.cpu(), batch_idxs_.cpu(), \n",
    "#                                                                     training_mode, using_set_aggr)             \n",
    "# proposals_idx[:, 1] = object_idxs[proposals_idx[:, 1].long()].int()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e58ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1490293f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c56c3316",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#         # return proposals_idx, proposals_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68970fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Forward grouping done\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6546b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if proposals_offset.shape[0] > model.train_cfg.max_proposal_num:\n",
    "#     proposals_offset = proposals_offset[:model.train_cfg.max_proposal_num + 1]\n",
    "#     proposals_idx = proposals_idx[:proposals_offset[-1]]\n",
    "#     assert proposals_idx.shape[0] == proposals_offset[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e546bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inst_feats, inst_map = model.clusters_voxelization(proposals_idx,\n",
    "#                                                   proposals_offset,\n",
    "#                                                   output_feats,\n",
    "#                                                   data[\"coords_float\"],\n",
    "#                                                   rand_quantize=True,\n",
    "#                                                   **model.instance_voxel_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60f8cfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iou_scores, mask_scores = model.forward_instance(inst_feats, inst_map,proposals_offset,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6b256a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instance_loss = model.instance_loss( mask_scores, \n",
    "#                                     iou_scores, \n",
    "#                                     proposals_idx,\n",
    "#                                     proposals_offset, \n",
    "#                                     data[\"instance_labels\"], \n",
    "#                                     data[\"instance_pointnum\"],\n",
    "#                                     epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f8d2e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses.update(instance_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5041d5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9f47f9",
   "metadata": {},
   "source": [
    "#Test Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c247c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['scan_ids', 'coords', 'batch_idxs', 'voxel_coords', 'p2v_map', 'v2p_map', 'coords_float', 'feats', 'semantic_labels', 'instance_labels', 'instance_pointnum', 'instance_cls', 'pt_offset_labels', 'spatial_shape', 'batch_size'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "517e368b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d1d7ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
